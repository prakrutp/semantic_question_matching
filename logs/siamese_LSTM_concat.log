Using TensorFlow backend.
2017-12-02 19:39:42.193535: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-02 19:39:42.193574: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-02 19:39:42.193581: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-02 19:39:42.193586: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-02 19:39:42.193591: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-02 19:39:42.320066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2017-12-02 19:39:42.320708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 0000:00:04.0
Total memory: 11.17GiB
Free memory: 11.09GiB
2017-12-02 19:39:42.320734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0
2017-12-02 19:39:42.320746: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y
2017-12-02 19:39:42.320757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0)
Number of train data instances read 323432
Number of test data instances read 80858
Obtained processed training data
Obtained embeddings
2017-12-02 19:40:30.280133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0)
siamese_model.py:67: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(256, dropout=0.2, recurrent_dropout=0.2)`
  lstm.add(LSTM(256, dropout_W=0.2, dropout_U=0.2))
siamese_model.py:76: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
  merged_output = merge([l_output, r_output], mode='concat')
/home/prakrutp/.local/lib/python2.7/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.
  name=name)
siamese_model.py:78: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, kernel_regularizer=<keras.reg..., activation="relu", bias_regularizer=<keras.reg...)`
  fcl = Dense(100, activation='relu', W_regularizer=l2(0.0001), b_regularizer=l2(0.0001))(merged_output)
siamese_model.py:79: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(50, kernel_regularizer=<keras.reg..., activation="relu", bias_regularizer=<keras.reg...)`
  fcl = Dense(50, activation='relu', W_regularizer=l2(0.0001), b_regularizer=l2(0.0001))(fcl)
siamese_model.py:81: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, kernel_regularizer=<keras.reg..., activation="relu", bias_regularizer=<keras.reg...)`
  fcl = Dense(25, activation='relu', W_regularizer=l2(0.0001), b_regularizer=l2(0.0001))(fcl)
siamese_model.py:84: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor("de..., inputs=[<tf.Tenso...)`
  model = Model(input=[l_input, r_input], output=prediction)
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to
==================================================================================================
input_1 (InputLayer)            (None, 40)           0
__________________________________________________________________________________________________
input_2 (InputLayer)            (None, 40)           0
__________________________________________________________________________________________________
sequential_1 (Sequential)       (None, 100)          29277268    input_1[0][0]
                                                                 input_2[0][0]
__________________________________________________________________________________________________
merge_1 (Merge)                 (None, 200)          0           sequential_1[1][0]
                                                                 sequential_1[2][0]
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 100)          20100       merge_1[0][0]
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 50)           5050        dense_2[0][0]
__________________________________________________________________________________________________
dense_4 (Dense)                 (None, 25)           1275        dense_3[0][0]
__________________________________________________________________________________________________
dense_5 (Dense)                 (None, 1)            26          dense_4[0][0]
==================================================================================================
Total params: 29,303,719
Trainable params: 622,519
Non-trainable params: 28,681,200
__________________________________________________________________________________________________
None
Built Model
Training now...
Train on 258745 samples, validate on 64687 samples
Epoch 1/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.5468 - acc: 0.7251Epoch 00001: val_acc improved from -inf to 0.76298, saving model to ../models/simple_lstm-01-0.76.hdf5
258745/258745 [==============================] - 301s 1ms/step - loss: 0.5468 - acc: 0.7251 - val_loss: 0.4888 - val_acc: 0.7630
Epoch 2/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.4726 - acc: 0.7720Epoch 00002: val_acc improved from 0.76298 to 0.78572, saving model to ../models/simple_lstm-02-0.79.hdf5
258745/258745 [==============================] - 313s 1ms/step - loss: 0.4726 - acc: 0.7720 - val_loss: 0.4491 - val_acc: 0.7857
Epoch 3/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.4317 - acc: 0.7956Epoch 00003: val_acc improved from 0.78572 to 0.79228, saving model to ../models/simple_lstm-03-0.79.hdf5
258745/258745 [==============================] - 323s 1ms/step - loss: 0.4317 - acc: 0.7956 - val_loss: 0.4361 - val_acc: 0.7923
Epoch 4/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.4031 - acc: 0.8123Epoch 00004: val_acc improved from 0.79228 to 0.80625, saving model to ../models/simple_lstm-04-0.81.hdf5
258745/258745 [==============================] - 322s 1ms/step - loss: 0.4031 - acc: 0.8123 - val_loss: 0.4147 - val_acc: 0.8063
Epoch 5/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.3798 - acc: 0.8249Epoch 00005: val_acc improved from 0.80625 to 0.81001, saving model to ../models/simple_lstm-05-0.81.hdf5
258745/258745 [==============================] - 309s 1ms/step - loss: 0.3798 - acc: 0.8249 - val_loss: 0.4105 - val_acc: 0.8100
Epoch 6/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.3579 - acc: 0.8373Epoch 00006: val_acc improved from 0.81001 to 0.81591, saving model to ../models/simple_lstm-06-0.82.hdf5
258745/258745 [==============================] - 307s 1ms/step - loss: 0.3578 - acc: 0.8373 - val_loss: 0.4060 - val_acc: 0.8159
Epoch 7/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.3394 - acc: 0.8473Epoch 00007: val_acc improved from 0.81591 to 0.81673, saving model to ../models/simple_lstm-07-0.82.hdf5
258745/258745 [==============================] - 303s 1ms/step - loss: 0.3394 - acc: 0.8473 - val_loss: 0.4022 - val_acc: 0.8167
Epoch 8/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.3232 - acc: 0.8571Epoch 00008: val_acc did not improve
258745/258745 [==============================] - 302s 1ms/step - loss: 0.3232 - acc: 0.8571 - val_loss: 0.4064 - val_acc: 0.8142
Epoch 9/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.3092 - acc: 0.8633Epoch 00009: val_acc did not improve
258745/258745 [==============================] - 302s 1ms/step - loss: 0.3092 - acc: 0.8633 - val_loss: 0.4210 - val_acc: 0.8124
Epoch 10/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.2969 - acc: 0.8698Epoch 00010: val_acc improved from 0.81673 to 0.81908, saving model to ../models/simple_lstm-10-0.82.hdf5
258745/258745 [==============================] - 302s 1ms/step - loss: 0.2968 - acc: 0.8698 - val_loss: 0.4134 - val_acc: 0.8191
Epoch 11/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.2867 - acc: 0.8753Epoch 00011: val_acc improved from 0.81908 to 0.82057, saving model to ../models/simple_lstm-11-0.82.hdf5
258745/258745 [==============================] - 305s 1ms/step - loss: 0.2867 - acc: 0.8753 - val_loss: 0.4153 - val_acc: 0.8206
Epoch 12/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.2745 - acc: 0.8816Epoch 00012: val_acc improved from 0.82057 to 0.82571, saving model to ../models/simple_lstm-12-0.83.hdf5
258745/258745 [==============================] - 306s 1ms/step - loss: 0.2745 - acc: 0.8816 - val_loss: 0.4105 - val_acc: 0.8257
Epoch 13/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.2675 - acc: 0.8855Epoch 00013: val_acc did not improve
258745/258745 [==============================] - 302s 1ms/step - loss: 0.2675 - acc: 0.8855 - val_loss: 0.4150 - val_acc: 0.8217
Epoch 14/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.2583 - acc: 0.8900Epoch 00014: val_acc did not improve
258745/258745 [==============================] - 305s 1ms/step - loss: 0.2583 - acc: 0.8900 - val_loss: 0.4285 - val_acc: 0.8226
Epoch 15/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.2524 - acc: 0.8933Epoch 00015: val_acc did not improve
258745/258745 [==============================] - 309s 1ms/step - loss: 0.2523 - acc: 0.8933 - val_loss: 0.4360 - val_acc: 0.8220
Epoch 16/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.2451 - acc: 0.8961Epoch 00016: val_acc did not improve
258745/258745 [==============================] - 306s 1ms/step - loss: 0.2451 - acc: 0.8961 - val_loss: 0.4649 - val_acc: 0.8044
Epoch 17/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.2392 - acc: 0.8991Epoch 00017: val_acc did not improve
258745/258745 [==============================] - 309s 1ms/step - loss: 0.2392 - acc: 0.8991 - val_loss: 0.4419 - val_acc: 0.8235
Epoch 18/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.2353 - acc: 0.9006Epoch 00018: val_acc did not improve
258745/258745 [==============================] - 307s 1ms/step - loss: 0.2353 - acc: 0.9006 - val_loss: 0.4345 - val_acc: 0.8236
Epoch 19/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.2300 - acc: 0.9034Epoch 00019: val_acc did not improve
258745/258745 [==============================] - 310s 1ms/step - loss: 0.2300 - acc: 0.9034 - val_loss: 0.4517 - val_acc: 0.8182
Epoch 20/30
258688/258745 [============================>.] - ETA: 0s - loss: 0.2263 - acc: 0.9057Epoch 00020: val_acc did not improve
258745/258745 [==============================] - 310s 1ms/step - loss: 0.2263 - acc: 0.9057 - val_loss: 0.4632 - val_acc: 0.8082

Metrics on test dataset
precision: [ 0.84732868  0.78571171]
recall: [ 0.88323976  0.72900789]
fscore: [ 0.86491162  0.75629844]
support: [50942 29916]
[[44994  8107]
 [ 5948 21809]]

Accuracy = (44994.0 + 21809.0) / (44994.0 + 5948.0 + 8107.0 + 21809.0) = 0.826176754310025